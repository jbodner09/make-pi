#Version 5: Never saw this one coming!, in `make_pi_5.cu`

You know what's more parallel than our processors nowadays?  Graphics cards!  So I figured, I have a discrete graphics card, why not write a version of the code for it?  It's got hundreds of little processors it can use compared to the four measly cores of my main processor, and it'll be fun learning to write code for a massively parallel architecture like all those big scientists do for curing cancer and stuff.  Now, we're really venturing into the territory of how they would do this in real life.  The interesting thing about graphics cards is that they really aren't just for graphics anymore.  They've got their own C-like language (many of them, in fact), and can really do just about anything a regular processor can, except usually faster.  

NVIDIA's CUDA programming paradigm is actually just an extention of C.  So, I used my Linux C code from Version 4, which used required surprisingly few modifications.  The biggest difference is the way threads are launched.  Other than using a few specific keywords in the thread function signature, I now don't have to launch them in a loop.  Instead, the GPU automatically knows how many to launch based on the way I've structured the division of the iterations.  

Instead of passing a discrete structure to each thread, I just pass the important parts one at a time in the function arguments.  This is because copying data to/from the GPU is a little more complicated.  Essentially, I can transfer entire arrays at a time, but those must be contiguous.  So, getting the resulting bignums back from the GPU was not trivial.  First, I create separate arrays to hold each part of the bignums on both the main system RAM and the GPU RAM.  So, one array for the power, one for the digits, etc.  Then, when each thread is done, I save each part of the resulting bignum in their respective arrays.  The arrays themselves, since they're created by the CPU, must be passed to the GPU kernel in the function arguments.  

Finally, once the kernels are done, I copy the arrays back to the system RAM and reconstruct each bignum as needed.  It technically requires no more memory than before, but instead of being chopped up into separate bignums and keeping track of the pointers, it's all contiguous arrays of individual parts.  The biggest difference now is that there are two completely separate sets of bignum functions:  one for use on the CPU, and one for use on the GPU.  The difference between each function is minimal.  The calls to malloc have a different signature than those on used on the CPU, and each function on the GPU has to be changed to reference other GPU functions, not their CPU counterparts.  

Other than that, the calculation kernel itself only needed to be modified to use its new arguments instead of the single object containing multiple arguments like before, as well as to use the GPU functions.  Additionally, saving each result from the kernel required the bignums to be split up into their separate parts for use in the arrays.  

Performance-wise, the GPU version behaves quite differently than the CPU version.  There is a lot more overhead, particularly when allocating memory on the GPU and copying the results back to the CPU.  The amount of cache available on the GPU is also less than on the CPU, so the GPU has to go out to memory more often, which can limit performance.  Overall, for a low number of iterations (less than 1 million), the overhead will cause the runtime to be higher than version 4 (CPU threading).  

For a larger number of iterations... the GPU version is still slower.  This may be slightly surprising, but it shouldn't be.  The amount of parallelism available in this code is high, but there is still a lot of work for each thread to perform.  Since each core of a GPU is usually clocked much slower than a standard CPU core, and CPU cores are designed specifically for low-latency between operations, each GPU core will take much, much longer to execute its few iterations compared to a CPU core's many iterations.  Plus, the GPU will probably have to go out to memory a lot more due to its comparatively smaller cache and more cores fighting for cache access.  Those factors together mean that the GPU code may actually run an order of magnitude SLOWER than the CPU version.  For example, on the Ohio Supercomputer Center's machines, 10,000,000 iterations on the GPU version takes 15 minutes, whereas on the CPU version (Update 3), it only took about 6 minutes!  Maybe, as a way around this, we could use... MPI?!?!?  On my own machine, the indirection through Visual Studio, along with the less powerful hardware, makes it UNGODLY slow for high numbers of iterations.  Like, it's bad.  

It's also worth noting that while this code will run on any GPU supporting CUDA 2.0+, its performance will need to be tuned to match the resources available.  The number of threads to use is held constant to be equal to the number of cores available on the particular architecture, which will change when running it on another GPU.  As the number of iterations increases, the amount of work each thread does increases, but to minimize switching, the number of threads still stays constant.  

##Version 5.1: Visual Studio helps me maintain better coding conventions, in `kernel.cu`

The Windows version is functionally identical in this case.  Well, you'll have to remove the reference to unistd.h, but it wasn't used in the Linux version anyway.  
