#Version 4: Maybe a little *too* excited, in `make_pi_4.c`

You know, writing your own math library is actually pretty tough.  After structuring my code in the last update to use a big number library, I figured I could pretty much use it verbatim and just drop in my own library's particulars.  That almost worked.  Almost.  I structured the names and signatures of my math functions to match GMP's.  In fact, besides not having their nice print function and being able to now specify the exact number of digits to use in my precision, the only real difference was that my functions weren't quite as user-friendly.  Specifically, I couldn't use the same operand as both the result and an input, which meant more temporary variables to hold results.  Also, I had to manually reset each variable before storing something new in it again.  So, a single operation turned into a set of operation-reset destination-move-reset temp block.  But other than those few gotcha's, writing the library wasn't too difficult.  

How did I go about doing it?  Well, it helped that I knew exactly which functions I needed to implement after seeing which ones I used in GMP's library.  It was basically just addition, multiplication, and division.  There were also functions to create a new big number, destroy it, and set its value, reset its value, and print it, as well as a few convenience functions that let me both create a temporary variable from a normal number then use it in an operation all in one step.  That helped keep the clutter in the main calculation function to a minimum.  Copying a big num into another was pretty simple, and printing was just an exercise in knowing if the power of the number was bigger or smaller than the number of digits it contains, which tells me if a decimal point is necessary, or if I need to pad with any zeros.  

I stored all that in a big number, as well as the maximum number of digits it could contain (which was necessary for array calculations in C), and the actual digits.  I decided to store the digits in decimal form, not binary.  They're all in an array of single-byte values.  So, the first array cell is the first decimal digit, the second cell is the second digit, etc.  Again, this making printing easy.  Obviously, there's still a limited number of digits you can store, but since you can set that number when you create a big num, if you run out of digits in one calculation, you can just re-do the calculation with more digits!  The only real downside is that since lots of the calculations involve looping through the entire list of digits, having more digits means the calculations take longer overall.  

To actually transform a regular number into a list of its digits, I took advantage of the fact that each digit is represented as a multiple of a power of ten:  the ones place is times 10^0, the tens place is times 10^1, the hundreds place is times 10^2, etc.  So, by taking the remainder of a division by 10, I get the current digit in the lowest place.  Then I divide by 10 to shift everything down and put the next digit in the lowest place.  That loop runs until the original number has no digits left, and the number of times I have to divide by 10 (minus one) is the power of the new number.  Then the only other thing I have to account for is not storing any extra zeros on the end.  Since I only support turning whole numbers into big nums, that's pretty much it.  Since this pi code doesn't even deal with negative numbers (no subtraction!), I could just assume everything is positive and then give the precondition for the function that only positive numbers are supported.  That also greatly simplifies most calculations.  Of course, the power of a number can be negative since that's how we eventually get decimal digits, but that's it.  

Of the three main functions to implement, addition was actually the trickiest.  It was the hardest to plan out ahead of time, and it ended up giving me the most bugs both during unit testing of the code on its own, as well as once I started using the library in the pi code (you should read that as:  I had the most array index bound errors here).  Floating point addition itself is actually pretty tricky (unless one of the inputs is zero, in which case I can just output the other input).  You have to shift over all the digits in one of the operands so that its power (decimal point) lines up with the other operand.  Then, you just add the numbers that are "on top of each other" like in normal addition.  

By convention, I decided to shift the number with the smaller power.  That meant shifting all its digits to the right because it will have less to the left of the decimal point than the number with the bigger power.  I tried to use a few shortcuts to just transfer the number of significant digits we had instead of just looping through every possible digit in the big num, but that led to a few (incorrect) assumptions about how many digits to actually add.  Those were fun bugs to find.  Addition is also interesting because if you shift the small operand too much, you'll start losing its digits due to the fact that you still have limited digit storage.  So, not only is this where error can be introduced, if you shift it too much, you'll actually lose it completely and just end up with the bigger operand as the answer!  

The power of the answer is then just the power of the bigger operand, and after shifting, the addition runs like you think it would:  add the digits in a column, and if there's carry, put it into the addition for the next column.  Keeping track of the number of significant digits (no trailing zeros) means detecting any time an addition in a right-most column ends up with a zero, and a final carry out means our power is actually incremented.  More importantly, a carry out means we have to shift our entire answer over by one since we store each digit of the answer initially assuming we won't have carry.  We do this so that the used digits within a big num always start at the leftmost slot in the digit storage array.  Again, this shifting can lead to lost precision, and was also the location of the second-to-last show-stopping bug I had to find during my porting to Windows (more on that later...).  

Multiplication was about as hard as addition to reason about ahead of time, but ended up being easier to implement due to the fact that there are a lot fewer "gotcha's" and a lot fewer places to get bugs due to running off a buffer.  The first interesting thing about multiplication is that you need up to twice as many digits internally to store the partial sums as you will have room in the answer.  By this I mean, since you keep shifting the sum from one of the operands over in each of the partial sums, you'll need the number of digits in one operand PLUS the number of digits in the other internally to store the partial sum.  If that number is greater than the number of digits you have to store the output in, then you'll end up losing some of the right-most (least significant) digits when you copy the final sum into the answer.  Again, source of error.  

Conceptually, multiplication is simple:  you take one digit in one operand, multiply every digit of the other operand by it, and add the result to the results from all the previous iterations of this step.  When you go on to the next digit in the first operand, you shift your partial sum over by one.  I decided to use the number with fewer significant digits as the "take one digit" part of the "and multiply it by every digit in the other operand" portion of the algorithm.  Really, it doesn't matter.  The power of the final answer is just the addition of the powers of the two input operands (one less if there's no carry out), and before any truncation due to limited storage, the number of digits in the answer equals the addition of the number of digits of the two operands.  

Like addition, we check for any zeros in the rightmost columns since we don't need to copy those over.  One other handy property of addition and multiplication is that they're commutative.  That means they only need one helper function to multiply or add a constant to a number:  no matter whether the constant is the left or right operand, the answer will be the same.  The same cannot be said for division:  dividing a number by a constant will give a different answer than dividing that constant by that number.  Multiplying by zero is also easier than addition since the answer is just zero.  Dividing by zero is caught (why are you doing that?...), and dividing zero by something is just zero.  

Overall, division has a few more tricks than multiplication, but somehow, it was the easiest to implement and ended up having the fewest bugs!  Maybe that's because I did it last and was smarter when I did it than when I did the first two.  I was actually kind of worried initially because, while I knew how to add and multiply floating-point numbers from classes, I couldn't think of how to do division.  Luckily, a quick trip to Wikipedia gave me plenty of algorithms, the simplest of which, upon second glance, worked out great.  The algorithm is essentially the opposite of multiplication:  when you multiply, you successively add one of the operands a certain number of times (that number is prescribed by the current digit in the other operand), and then shift the answer to the left for the next digit of the other operand.  In division, you subtract successively and then shift to the right.  

So, that meant I had to implement both comparison and subtraction.  The subtraction was easy, since it just used the addition algorithm, but instead of carrying from the next column over, it borrowed from it.  The comparison was because what you essentially do is subtract the denominator from the current numerator a certain number of times until doing so would take the numerator to less than zero.  Obviously, since I don't support negative numbers, I had to catch that before it happened.  So, you subtract until the NEXT subtraction will take it below zero.  The number of times you've subtracted becomes the current digit of the answer, and then you shift the denominator to the right, adding an extra digit to the numerator.  This "subtract until you can't anymore" loop runs either until we run out of significant digits in the answer (truncating our result, leading to error), or if the numbers divide evenly, the numerator is exactly zero.  Thus, we need an internal word that's twice as long here as well.   

The comparison is straightforward:  look at the leftmost digit, and it'll tell you which number is bigger or smaller.  If they're the same, move to the next digit to the right.  The basic guarantee is that we only perform a subtraction if the current numerator is at least as big as the denominator, so the subtraction won't give us a negative number.  Multiplication just gave us a shortcut:  we didn't have to do successive addition because we could just directly multiply each digit, and the carry would propagate over to the next digit like we expected.  The power of the answer for division is now just the power of the numerator MINUS the power of the denominator.  Overall, you may find that this code is a little more... beefy than anything that's come before it due to all the math functions.  The actual pi calculation stuff is pretty much the same, though!  

Will this be the final update?

##Version 4.1: No, in `WinPi2.cpp`

Oh. My. God.  That port to Windows was such a ridiculous amount of work.  As it turns out, all the work stemmed from the bugs that still lingered in the code.  Let me explain.  On Linux, everything worked fine the way I had it.  Unfortunately, the way I had it left me accessing tons of arrays outside of their bounds.  Obviously that's bad, but for some reason it let me!  Only one of the out-of-bounds accesses was a write, which Linux just ignored, and it didn't make my result wrong enough for me to notice.  So, after changing the few things I needed to in order to use the Windows threading APIs, I thought I could just compile it and it would work.  It didn't.  Instead, it crashed.  

Turns out, Windows isn't as lenient about writing outside of array bounds.  Either that, or the Visual Studio compiler is better about detecting and alerting me to such things.  Now, that one seemed pretty tricky to find, since it only alerted me to the error once I tried to free the offending array.  Since that means I didn't know where it actually happened, I had to go back and analyze every single time I wrote to that array until I eventually found the offending index.  So, fixed the bad write, no problem, right?  Wrong.  Now, it was completing, but I was getting wrong results!  

Okay, what gives?  The only other thing I changed was the fact that Visual Studio was alerting me to comparing signed and unsigned integers, which I fixed.  Well, remember that, because it'll come back later.  For some reason, I started getting negative numbers popping up.  Now, that shouldn't happen at all in these calculations since we're not subtracting anything, so something is definitely up.  Well, I actually had a LOT of reading off the edges of arrays as well.  Windows is actually as lenient about this as Linux is:  it just does it, and if you read a bad value, well, that's your problem!  

So, I go through, and I actually find a bunch of places that are either directly offending, or they could if I were doing different calculations that stretched the use of these big numbers differently.  After wrapping them in checks to avoid bad array accesses, as well as fixing a few other bugs (or hiding them, in a few cases. Hey, what I don't know won't hurt me, right?), there was still one bug that was plaguing me.  I can't for the life of me find where this last bad read is coming from.  

Well, it turns out that one of the checks I had for finding bad array accesses was actually using some of those newly-unsigned integers I had fixed!  In Linux, they were just implied to be signed, and the check ran fine (not to mention, it ran fine before I even added the check).  But, when I removed the sign for Windows, the check no longer triggered, and so even though I had found a bad spot, I had inadvertently undone the fix with another fix!  Finding that took... a lot longer than I had hoped.  After undoing that, along with another fix that was actually bogus, I finally got the port working, and it turns out that it run WAAAAAY faster on my awesome computer than on the school's Linux servers.  I still haven't tried it on the Supercomputer, and I may never get to, but the fact that I can now at least do it on my own machines is a very important step.  And even though there are probably still tons of bugs in it, I'll probably never touch the code again, except to post it each year.  

##Version 4.2: How did I have this much free time?, in `Program.cs`

You know, after porting to Windows gave me so much trouble, I figured porting to C\# would be about as much of a hassle.  I wasn't too far off.  And it's not because figuring out how to transform all my array magic and pointer stuff into managed code is difficult.  It's because C's native code is so lenient about out-of-bounds array access.  C's explicit contract is, "You will stay within the bounds of an array.  If you don't, ANYTHING CAN HAPPEN.  We literally leave it up to each individual implementation on each individual platform to care whether or not you meet this requirement and what happens if you don't".  

Managed code is not as friendly.  They have absolutely no leniency when it comes to staying within array bounds.  So, all those read-outside-the-array errors I should have fixed when I fixed all the write-outside-the-array errors but didn't?  Well, I had to now.  I guess it's not a bad thing that the code is "more correct" now, and actually, wrapping a lot of those functions inside classes means the actual pi code that uses them doesn't have to account for so many quirks like the C code did since I can build in a few more shortcuts with relative ease.  Also, managed code makes debugging WAY easier.  It's so great to know exactly where I mangled an array instead of just being presented with a call stack that has nothing to do with where the error actually occurred and an error that's like, "lol, you figure it out!".  

Overall, since I found most of the bad array accesses when porting to Windows, most of the hard work was already done.  That stupid signed-versus-unsigned thing still got me, though.  

##Version 4.3: Well, why not?, in `Program.java`, `Calculation.java`, and `BigNum.java`

This time, getting from C\# to Java was a piece of cake.  Of... coffee cake???
